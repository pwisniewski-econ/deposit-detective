---
format: 
  pdf:
    number-sections: true
    block-headings: false
    fig-format: pdf
    code-block-border-left: "#5b5b5b"
    code-block-bg: "#fafafa "
    highlight-style: pygments
    documentclass: article
    toc: false
    toc-depth: 2
    toccolor: black
    citecolor: black
    urlcolor: gray
    fontsize: "12pt"
    include-before-body: 
      - text: |
          \input{ressources/title-page/title-page.tex}
    include-in-header:
      - text: |
          \usepackage{graphicx}
          \usepackage{pdflscape}
          \usepackage{pdfpages}
          \newcommand*{\boldone}{\text{\usefont{U}{bbold}{m}{n}1}}
          \usepackage[a4paper, portrait, footnotesep=0.75cm, margin=2.54cm]{geometry}
          \usepackage{enumitem}
          \usepackage{parskip}
          \usepackage{titling}
          \linespread{1.5}
          \usepackage[T1]{fontenc}
          \usepackage[hidelinks]{hyperref}
          \hypersetup{linkcolor={black}}
          \usepackage{amsmath}
          \usepackage{amsfonts}
          \usepackage[normalem]{ulem}
          \usepackage{times}
          \usepackage{sectsty}
          \usepackage{ressources/tikz/tikzit}
          \input{ressources/tikz/tikz.tikzstyles}
          \newcommand{\ts}{\textsuperscript}
    pdf-engine: pdflatex
---
 
```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(cowplot)
library(data.table)
library(sf)
library(showtext)
library(here)
library(arrow)
font_add(family = "Times", regular = "ressources/fonts/Times-New-Roman.otf")
showtext_auto()
showtext_opts(dpi = 300)

knitr::opts_chunk$set(echo = F, warning = F, error = F, message = F)
```

\clearpage

# Introduction and Relevant Literature


\newpage

# Data

## Marketing Campaign

\begin{figure}
\centering
\caption{Campaign Process and Observed Data}
\tikzfig{ressources/tikz/fig1}
\end{figure}


\newpage
## Descriptive Statistics

```{r}
count_prop <- function(DATA){
  DATA |> summarise(`Count` = n()) |>
  mutate(`Share (%)` = `Count`/sum(`Count`) * 100)
}
```



```{r}
DATA_ADD_FULL <- read_feather("results_building/bank-additional-full.feather") |>
  mutate(outcome = if_else(y==1, "Sucess", "Failure"))
DATA_FULL <- read_feather("results_building/bank-full.feather")
```

```{r}
DATA_ADD_FULL |>
  group_by(Outcome = outcome) |>
  count_prop() |>
  knitr::kable(digits = 2, align = "c", caption = "Observed Outcomes")
```






```{r, fig.cap= "Probability of sucess and number of calls", fig.height=3.5, fig.width=5.5}
DATA_ADD_FULL |>
  mutate(
    `Last observed call` = if_else(campaign>8, "9+", as.character(campaign))
  ) |>
  group_by( `Last observed call`, outcome)|>
  summarise(count = n()) |>
  ggplot(aes(x=`Last observed call`, y=count, fill = outcome))+
  geom_bar(stat="identity") +
  theme_minimal()+
  theme(text = element_text(family = "Times"), 
        legend.position = "bottom", 
        legend.margin = margin(-10,0,0,0),
        legend.title = element_text(size = 9), legend.text = element_text(size = 8))+
  labs(x= "", fill="", y= "Number of Calls", caption = "Bank Marketing, PT, 2008-2010")+
  guides(fill = guide_legend(override.aes = list(size = 0)))+
  scale_fill_brewer(palette = "Blues")
```


```{r, fig.cap= "Number of observed last calls", fig.height=3.5, fig.width=5.5}
DATA_ADD_FULL |>
  group_by(date_month, outcome)|>
  summarise(count = n()) |>
  ggplot(aes(x=date_month, y=count, fill = outcome))+
  geom_bar(stat="identity") +
  theme_minimal()+
  theme(text = element_text(family = "Times"), 
        legend.position = "bottom", 
        legend.margin = margin(-10,0,0,0),
        legend.title = element_text(size = 9), legend.text = element_text(size = 8))+
  labs(x= "", fill="First Call", y= "Number of Calls", caption = "Bank Marketing, PT, 2008-2010")+
  guides(fill = guide_legend(override.aes = list(size = 0)))+
  scale_fill_brewer(palette = "Blues") 
```



```{r, fig.cap= "Euribor 12mo rates", fig.height=3, fig.width=5.5}
DATA_ADD_FULL |>
  select(date_month, euribor_12mo) |>
  unique() |>
  ggplot(aes(x = date_month, y=euribor_12mo))+
  geom_line()+
  theme_minimal()+
  theme(text = element_text(family = "Times"), 
        legend.position = "bottom", 
        legend.margin = margin(-10,0,0,0),
        legend.title = element_text(size = 9), legend.text = element_text(size = 8))+
  labs(x= "", y= "Interest Rate (%)", caption = "Euribor data, 2008-2010")
```



```{r,  fig.cap= "Euribor 12mo rates", fig.height=5, fig.width=5}
numeric_data <- DATA_ADD_FULL |>
  select(where(is.numeric)) |>
  select(-c(hicp, stoxx_return, cons_confidence, euribor_12mo, nr.employed, euribor3m, cons.conf.idx, cons.price.idx, emp.var.rate, mid_week))

cor_matrix <- cor(numeric_data)
#corrplot(numeric_data_wo_na)
ggcorrplot::ggcorrplot(cor_matrix, type = "lower")+
  theme(text=element_text(family = "Times"))
```


\newpage
## Exploratory Analysis

```{r}
DATA_ADD_FULL |>
  mutate(Share = if_else(y==1, "Sucess", "Failure"), day_of_week = factor(
    day_of_week, 
    levels = c("mon", "tue", "wed", "thu", "fri"), 
    labels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
  )) |>
  group_by(day_of_week, Share) |>
  summarise(cnt = n()) |>
  mutate(prop = cnt/sum(cnt)) |>
  ungroup() |>
  select(-cnt) |>
  pivot_wider(names_from = "day_of_week", values_from = "prop") |>
  knitr::kable(digits = 3, align = "c", caption = "Share of sucessful calls")
```

\newpage

# Methodology and Objectives

The objective of this project is to estimate the causal effect of our treatment of study. We consider three experiments.

First, we consider seek to estimate the causal effect of being contacted in the middle of the week by the bank, versus another day. We hypothesis there is a positive causal effect to calling people not on Mondays nor on Fridays, because the beginning of the week is often associated with rush and catching-up on previous week's work , while the end of the week is associated with meeting deadlines before the weekend. Thus,  someone called in the middle of the weeks is more inclined to accept the term deposit product.

Second and relatedly, we are interested in the effect of being contacted at the end of the month, versus at the beginning. The reasoning is that at the beginning of the month, people often have more cash at hand from their revenue (assumed to be received towards the end of the month) and is thus more inclined to accept the deposit.

In a third experiment, we are interested in the effect of the Euro Interbank Offered Rate (Euribor) on the probability of take-up. Since interest rates on term deposits react quickly to the Euribor, they reflect the rate set by the European Central Bank. Based on this observation, we hypothesise that customers reached when the Euribor is high have a higher probability of subscribing to the term deposit. 


## Data pre-processing

In this section we detail the steps occuring in our data preprocessing pipeline.

Some variables are numeric such as pdays (the number of days since called from a previous campaign) but take special values for some cases. For instance, pdays = 999 if the person was never contacted from a preivous campaign, but is otherwise = 1, 2, ... 26. 

```{r}
# pdays (nb days since called from a previous campaign)
pdays = DATA_ADD_FULL$pdays
pdays = pdays[pdays != 999]

# Plot histogram of pdays
hist(pdays, breaks = seq(min(pdays), max(pdays)),
     main = "Histogram of pdays (Bin size = 5)",
     xlab = "Values", 
     col = "skyblue", 
     border = "black")

summary(pdays) 

#  we transform pdays into a categorical variable, with 3 categories:
# never called (previously 999)
# called less than or equal to 6 days ago (the median)
# called more than 6 days ago
DATA_ADD_FULL$pdays <- as.factor(ifelse(DATA_ADD_FULL$pdays == 999, 
                                  "Never Contacted",
                                  ifelse(DATA_ADD_FULL$pdays <= 6, 
                                         "<=6 days ago", 
                                         ">6 days ago")))
str(DATA_ADD_FULL$pdays)

```

In order to achieve unconfoundedness, that is random assignment of the treatment conditional on covariates, we only consider observations for which campaign = 1, that is contacted only once during the present campaign. Indeed, campaign > 1 means that someone has requested to reschedule the call for later. This poses an issue of self-selection into the treatment if we consider our experiment where being treated means  being contacted during the middle of the week, as people may reschedule a call next Wednesday for instance, if  they expect to be less busy.  


```{r}
# keep only observation called once to ensure treatment is random given covariates
dataset = dataset %>% filter(campaign == 1)
dataset <- subset(dataset, select = -campaign)

```

We convert date_month from date to factor in order create dummies for our models.

```{r}
# convert date_month to categorical
dataset$date_month = as.factor(dataset$date_month) 
```

Finally we look at outcome imbalance. We find a majority of "no" (around 85\%) but we consider the imbalance as not too severe and therefore proceed without attempting to mitigate this imbalance.  

```{r}
# imbalanced outcome
prop.table(table(dataset$outcome))
```

\newpage

# Mid-week calls

During preliminary data exploration, we have identified an association between being called in the middle of the week and the probability of accepting the term deposit. We suspect a positive causal effect, and investigate in this section.

First we look at covariate balance, to check if treatment is assigned evenly across the treatment and control groups. We find that for numeric variables, control and treatment groups have similar standardised means, and similarly, categorical variables have roughly the same share of people across of their categories in the control and treatment groups. This allows us to conclude that the distribution of treatment is quite balanced across control and treatment groups, and that we can proceed safely with our models.

```{r}

# Covariate Balance Table: checking
covariates = colnames(dataset)[colnames(dataset) != "treatment"]
covariates = covariates[covariates != "outcome"]

table1 <- CreateTableOne(vars = covariates, strata = "treatment", data = dataset, test = FALSE)
print(table1, smd = TRUE)

# distribution of treatment is quite balanced across control and treatment groups
# we can proceed safely with logit

```

## PICO process


**Population**: Customers from a Portuguese bank, who are called as part of a telemarketing campaign for a term deposit product

**Intervention**: Receiving a call during the middle of the week (Tue, Wed, Thur)

**Comparison**: Customers called a Monday or Friday

**Outcome**: Subscribing or not to the term deposit

**Time**: Outcome is determined after the call ends (directly after treatment)

## DAG and Hypothesis

Importantly, since $duration$ (the duration of the last call) is only recorded after the end of the call, that is after the outcome is known, we classify it as a collider or post-treatment variable. Therefore, we drop it from our covariates list.

```{r}
# removing post-treatment variables
dataset <- subset(dataset, select = -duration)
```

## Models

The models we consider are the logit, the Augmented IPW, 

Our set of controls therefore include all socio-demographic variables, month dummies from May 2008 until November 2010, and macro variables (from the original  dataset + added ones).

### Logit 

We use the Logit as a benchmark for the rest of our models.


```{r}
###################################################### LOGIT

# Strategy: incorporate month dummies and remove all macro variables
# to prevent instability of coefficients due to correlation
# Month dummies capture economic recovery, and any effect related to the current state of the world in Portugal at the time of the call
data_logit <- subset(dataset, select = - c(euribor_12mo, hicp, cons_confidence, unemployment, stoxx_return))
str(data_logit)

logit_model <- glm(outcome ~ treatment + .,
                   data = data_logit, family = binomial)
```

### Augmented IPW

Then we experiment with the AIPW estimator or doubly robust learning estimator. This estimator combines the G-formula (conditional mean regression) and the IPW (where the outcome is  weighted by the inverse of the propensitiy score), and only requires one to be well-specified. 
```{r}

################################################### AIPW
str(data_logit)

data_ipw = data_logit
W = subset(data_logit, select = -c(outcome, treatment))
X_numeric <- model.matrix( ~ . - 1, data = W)  # "-1" removes the intercept column
str(X_numeric)


#create an object
aipw_sl <- AIPW$new(Y=data_logit$outcome, 
                    A=data_logit$treatment,
                    W=X_numeric,
                    Q.SL.library = c("SL.glm", "SL.mean"),
                    g.SL.library = c("SL.glm", "SL.mean"),
                    k_split=10,verbose=TRUE)
#fit the object
aipw_sl$stratified_fit()
```

## Results 

The logit is a  benchmark for our ML models.

We find a coefficient of 0.21 significant at 0.001%. To validate our model and make sure overfit is not an issue, we assess the performance of our binary classifier with the ROC curve and the AUC metric. $Sensitivity$ is the true-positive rate while $1-specificity$ is the false positive rate. We find and an AUC of 0.82 indicating high predictive performance, confirming overfitting is not an issue.

Our coefficient for $treatment$ translates into an average marginal effect of 0.0198.

```{r}
###################################################### LOGIT
# Print summary of the model
summary(logit_model)
exp(coef(logit_model))

# Analysis of results
pred_logit <- predict(logit_model, data = dataset, type = "response")

# Generate ROC Curve and calculate AUC
roc_obj <- roc(dataset$outcome, pred_logit)
auc_value <- auc(roc_obj)

# Generate Confidence Interval
ci_obj <- ci.auc(roc_obj)
print(paste("AUC Confidence Interval:", round(ci_obj[1], 3), "-", round(ci_obj[3], 3)))

# Plotting the ROC Curve with ggplot
ggplot() +
  geom_line(aes(x = 1-roc_obj$specificities, y = roc_obj$sensitivities), color = "blue", size = 1.2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = paste("ROC Curve (AUC =", round(auc_value, 3), 
                     "with CI:", round(ci_obj[1], 3), "-", round(ci_obj[3], 3), ")"),
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal()

# Comment on the results
# The coefficient on treatment is significant at the 0.001% confidence level
# 
# The area under the curve is around 0.8 which reveals that the model is good and 
# is able to discriminate between individuals subscribing upon treatment and those who do not


######################### ATE ########################################


# Average Marginal Effect
ame <- margins(logit_model, variables = "treatment")
summary(ame)
```


Next we show the ATE found by the AIPW estimator.

We find an ATE (Risk difference between the treated and control groups) of 

To compare the ATE, we compare the AME from the logit and the risk difference from the AIPW. We find that the estimators find treatment effects in the same zone, around 0.2.
That is, someone called in the middle of the week is 0.2 percentage point more likely to subscribe to the term deposit.

```{r}

################################################### AIPW
#calculate the results
aipw_sl$summary(g.bound = 0.025)
#check the propensity scores by exposure status after truncation
aipw_sl$plot.p_score()
print(aipw_sl$result, digits = 5)

# recompute LOGIT coefficient
# Values from your output
p0 <- aipw_sl$result["Risk of control", "Estimate"]  # Risk under control
att_rd <- aipw_sl$result["ATT Risk Difference", "Estimate"]
p1 <- p0 + att_rd

# Compute odds
odds0 <- p0 / (1 - p0)
odds1 <- p1 / (1 - p1)

# Compute odds ratio and logit coefficient
or <- odds1 / odds0
logit_coef <- log(or)

# Output
cat("Implied OR:", round(or, 4), "\n")
cat("Implied Logit Coefficient (log OR):", round(logit_coef, 4), "\n")


## plot 
# From logistic regression
logit_est <- coef(logit_model)["treatment"]
logit_se <- summary(logit_model)$coefficients["treatment", "Std. Error"]
logit_lower <- logit_est - 1.96 * logit_se
logit_upper <- logit_est + 1.96 * logit_se

# From AIPW - let's say you’re comparing log OR
# Use your actual values here
aipw_log_or <- log(aipw_sl$result["Odds Ratio", "Estimate"])  # OR from AIPW output
aipw_se <- aipw_sl$result["Odds Ratio", "SE"] / aipw_sl$result["Odds Ratio", "Estimate"]  # Delta method: SE(log OR) ≈ SE(OR) / OR
aipw_lower <- aipw_log_or - 1.96 * aipw_se
aipw_upper <- aipw_log_or + 1.96 * aipw_se

# Create comparison data frame
df <- tibble(
  method = c("Logit", "AIPW"),
  estimate = c(logit_est, aipw_log_or),
  lower = c(logit_lower, aipw_lower),
  upper = c(logit_upper, aipw_upper)
)

# Plot
ggplot(df, aes(x = method, y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15) +
  labs(title = "Treatment Effect (log OR): Logit vs AIPW",
       y = "Log Odds Ratio", x = "") +
  theme_minimal()

## Marginal effects
# 1. Extract marginal effect for "treatment" from logit model
ame_summary = summary(ame)
logit_marginal <- ame_summary[ame_summary$factor == "treatment", ]
logit_est <- logit_marginal$AME
logit_se <- logit_marginal$SE
logit_lower <- logit_est - 1.96 * logit_se
logit_upper <- logit_est + 1.96 * logit_se

# 2. Extract AIPW Risk Difference (ATE) from your object
aipw_result <- aipw_sl$result
aipw_rd_row <- aipw_result["Risk Difference", ]
aipw_est <- aipw_rd_row["Estimate"]
aipw_se <- aipw_rd_row["SE"]
aipw_lower <- aipw_est - 1.96 * aipw_se
aipw_upper <- aipw_est + 1.96 * aipw_se

# 3. Combine into a tidy data frame
df_plot <- tibble(
  Method = c("Logit AME", "AIPW Risk Difference"),
  Estimate = c(logit_est, aipw_est),
  Lower = c(logit_lower, aipw_lower),
  Upper = c(logit_upper, aipw_upper)
)

# 4. Plot
ggplot(df_plot, aes(x = Method, y = Estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.15) +
  labs(title = "Average Treatment Effect",
       y = "ATE (Probability Scale)", x = "") +
  theme_minimal()

```


## Heterogeneity Analysis

The logit finds that the treatment effect is particularly large and significant for  retired people and students, among the bank's customer base. This finding is opposite that what we hypothesised, meaning that it is in fact people for whom work does not necessarily need to be delivered on Fridays or to be caught up on Mondays that react more positively to the call.

```{r}

# Heterogeneous
marital_ame <- margins(logit_model, variables = "treatment", at = list(marital = c("married", "single")))
summary(marital_ame)

me_by_job <- margins(logit_model, variables = "treatment", at = list(job = unique(data_logit$job)))
me_by_job = summary(me_by_job)

# plot 

me_df <- as.data.frame(me_by_job)
me_df$lower <- me_df$AME - 1.96 * me_df$SE
me_df$upper <- me_df$AME + 1.96 * me_df$SE

ggplot(me_df, aes(x = reorder(job, AME), y = AME)) +
  geom_point(size = 2) +  # Point estimate
  geom_linerange(aes(ymin = lower, ymax = upper), size = 0.6) +  # CI bounds
  coord_flip() +  # Horizontal layout
  labs(
    x = "Job Category",
    y = "Marginal Effect of Treatment (AME)",
    title = "Heterogeneous Treatment Effect by Job"
  ) +
  theme_minimal(base_size = 13)


```


```{r, fig.align='center', fig.height=4, fig.width=7.25, message=FALSE, warning=FALSE, fig.cap="Change in take-up in middle of the week"}
WEEKDAYS_results <- read_feather("results_analysis/weekdays_causal_forest.feather")

WEEKDAYS_results |> 
  filter(name!="causal-forest-all") |>
  mutate(name = str_remove_all(name, "causal-forest-")) |>
  ggplot(aes(x=name, y=estimate, ymin = ci_lower, ymax = ci_upper))+
  geom_point()+
  geom_errorbar(width=0.5)+
  labs(x = "", y = "Coefficient", title="", caption = "Group level analysis, PT, 2008-2010")+
  theme_minimal()+
  theme(text = element_text(family = "Times"))+
  geom_hline(yintercept = 0, linetype="dotted")
```

## Robustness checks

We want to ensure that the treatment effects we find are not too sensitive to covariate choice.

### Logit

We run an experiment on the Logit by dropping month dummies and instead using our macro variables. We find a similar coefficient on $treatment$, also significant at the 1\% level.

Next we turn to including all covariates, including month dummies and all macro variables. Our estimates remain robust to this addition.
```{r}


### ##################################################
#ROBUSTNESS
##################################################


###################################################### Logit without month dummies but with macro 

data_logit <- subset(dataset, select = - c(date_month))
str(data_logit)

# Run logistic regression
# we put date_month as factor to act as month fixed effects, thereby capturing
# economic recovery, and any effect related to the current state of the world in 
# Portugal at the time of the call
logit_model <- glm(outcome ~ treatment + .,
                   data = data_logit, family = binomial)

# Print summary of the model
summary(logit_model)
exp(coef(logit_model))

# Analysis of results
pred_logit <- predict(logit_model, data = dataset, type = "response")

# Generate ROC Curve and calculate AUC
roc_obj <- roc(dataset$outcome, pred_logit)
auc_value <- auc(roc_obj)

# Generate Confidence Interval
ci_obj <- ci.auc(roc_obj)
print(paste("AUC Confidence Interval:", round(ci_obj[1], 3), "-", round(ci_obj[3], 3)))

# Plotting the ROC Curve with ggplot
ggplot() +
  geom_line(aes(x = 1-roc_obj$specificities, y = roc_obj$sensitivities), color = "blue", size = 1.2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = paste("ROC Curve (AUC =", round(auc_value, 3), 
                     "with CI:", round(ci_obj[1], 3), "-", round(ci_obj[3], 3), ")"),
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal()

############################################# logit with all covariates
data_logit <- dataset
logit_model <- glm(outcome ~ treatment + .,
                   data = data_logit, family = binomial)

# Print summary of the model
summary(logit_model)
exp(coef(logit_model))

```

### AIPW

We also run an experiment on the AIPW where we drop month dummies to include macro variables. This yields very similar treatment effect estimates.
```{r}
####################################################  AIPW without month dummies with macro
data_logit <- subset(dataset, select = - c(date_month))
str(data_logit)

data_ipw = data_logit
W = subset(data_logit, select = -c(outcome, treatment))
X_numeric <- model.matrix( ~ . - 1, data = W)  # "-1" removes the intercept column
str(X_numeric)


#create an object
aipw_sl <- AIPW$new(Y=data_logit$outcome, 
                    A=data_logit$treatment,
                    W=X_numeric,
                    Q.SL.library = c("SL.glm", "SL.mean"),
                    g.SL.library = c("SL.glm", "SL.mean"),
                    k_split=10,verbose=TRUE)
#fit the object
aipw_sl$stratified_fit()
#calculate the results
aipw_sl$summary(g.bound = 0.025)
#check the propensity scores by exposure status after truncation
aipw_sl$plot.p_score()
print(aipw_sl$result, digits = 5)

# recompute LOGIT coefficient
# Values from your output
p0 <- aipw_sl$result["Risk of control", "Estimate"]  # Risk under control
att_rd <- aipw_sl$result["ATT Risk Difference", "Estimate"]
p1 <- p0 + att_rd

# Compute odds
odds0 <- p0 / (1 - p0)
odds1 <- p1 / (1 - p1)

# Compute odds ratio and logit coefficient
or <- odds1 / odds0
logit_coef <- log(or)

# Output
cat("Implied OR:", round(or, 4), "\n")
cat("Implied Logit Coefficient (log OR):", round(logit_coef, 4), "\n")


## plot 
# From logistic regression
logit_est <- coef(logit_model)["treatment"]
logit_se <- summary(logit_model)$coefficients["treatment", "Std. Error"]
logit_lower <- logit_est - 1.96 * logit_se
logit_upper <- logit_est + 1.96 * logit_se

# From AIPW - let's say you’re comparing log OR
# Use your actual values here
aipw_log_or <- log(aipw_sl$result["Odds Ratio", "Estimate"])  # OR from AIPW output
aipw_se <- aipw_sl$result["Odds Ratio", "SE"] / aipw_sl$result["Odds Ratio", "Estimate"]  # Delta method: SE(log OR) ≈ SE(OR) / OR
aipw_lower <- aipw_log_or - 1.96 * aipw_se
aipw_upper <- aipw_log_or + 1.96 * aipw_se

# Create comparison data frame
df <- tibble(
  method = c("Logit", "AIPW"),
  estimate = c(logit_est, aipw_log_or),
  lower = c(logit_lower, aipw_lower),
  upper = c(logit_upper, aipw_upper)
)

# Plot
ggplot(df, aes(x = method, y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15) +
  labs(title = "Treatment Effect (log OR): Logit vs AIPW",
       y = "Log Odds Ratio", x = "") +
  theme_minimal()

## Marginal effects
# 1. Extract marginal effect for "treatment" from logit model
ame_summary = summary(ame)
logit_marginal <- ame_summary[ame_summary$factor == "treatment", ]
logit_est <- logit_marginal$AME
logit_se <- logit_marginal$SE
logit_lower <- logit_est - 1.96 * logit_se
logit_upper <- logit_est + 1.96 * logit_se

# 2. Extract AIPW Risk Difference (ATE) from your object
aipw_result <- aipw_sl$result
aipw_rd_row <- aipw_result["Risk Difference", ]
aipw_est <- aipw_rd_row["Estimate"]
aipw_se <- aipw_rd_row["SE"]
aipw_lower <- aipw_est - 1.96 * aipw_se
aipw_upper <- aipw_est + 1.96 * aipw_se

# 3. Combine into a tidy data frame
df_plot <- tibble(
  Method = c("Logit AME", "AIPW Risk Difference"),
  Estimate = c(logit_est, aipw_est),
  Lower = c(logit_lower, aipw_lower),
  Upper = c(logit_upper, aipw_upper)
)

# 4. Plot
ggplot(df_plot, aes(x = Method, y = Estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.15) +
  labs(title = "Average Treatment Effect",
       y = "ATE (Probability Scale)", x = "") +
  theme_minimal()

```



\newpage

# Time of the month

## PICO process

**Population**: Customers from a Portuguese bank, who are called as part of a telemarketing campaign for a term deposit product

**Intervention**: Receiving a call at the end of the month (after the 16th of each month)

**Comparison**: Customers called a Monday or Friday

**Outcome**: Subscribing or not to the term deposit

**Time**: Outcome is determined after the call ends (directly after treatment)

## DAG and Hypothesis

## Results

## Heterogeneity Analysis

```{r, fig.align='center', fig.height=4, fig.width=7.25, message=FALSE, warning=FALSE, fig.cap="Change in take-up at the end of the month"}
EOM_results <- read_feather("results_analysis/eom_causal_forest.feather")

EOM_results |> 
  filter(name!="causal-forest-all") |>
  mutate(name = str_remove_all(name, "causal-forest-")) |>
  ggplot(aes(x=name, y=estimate, ymin = ci_lower, ymax = ci_upper))+
  geom_point()+
  geom_errorbar(width=0.5)+
  labs(x = "", y = "Coefficient", title="", caption = "Group level analysis, PT, 2008-2010")+
  theme_minimal()+
  theme(text = element_text(family = "Times"))+
  geom_hline(yintercept = 0, linetype="dotted")
```

\newpage

# Macro Context

This section analyzes macroeconomic determinants of the marketing campaign, focusing on the impact of interest rates on product take-up rates. The specific rate offered to consumers is unobserved; however, the product, a term deposit, is among the fastest to react to changes in market conditions. The 12-month EURIBOR rate is used as a proxy, given its strong predictive power for such products. The objective is to estimate consumer demand elasticity with respect to interest rates. This has two key implications for the offering bank: it informs the expected effect of offering a more competitive rate or supports assessment of campaign viability when pricing power is limited and rates are determined by market conditions.

## PICO summary

**Population**: The population of interest consists of bank consumers; however, due to previously mentioned data limitations, the analysis is restricted to clients who participated in the marketing campaign. To simplify the study design, only clients who received a single call are included.

**Intervention**: Variation in market interest rates which was particularly pronounced during the 2008–2010 period is leveraged to estimate demand elasticity.

**Comparison**: Clients exposed to lower market interest rates serve as a control group for those exposed to higher interest rates.

**Outcome**: The primary outcome of interest is the take-up of the offered savings product, measured as a change in the take-up rate.

## DAG and Hypothesis

The main challenge in identifying the effect of interest rates on take-up rates is endogeneity. Specifically, interest rates represent market prices for capital and are thus the joint outcome of both supply and demand.

This is problematic because interest rates are correlated with demand shocks, which directly influence take-up behavior. A clear example is the bankruptcy of Lehman Brothers, which marked the onset of the great financial crisis. This significantly altered investor risk preferences, leading to a substantial inflow of capital into safer financial instruments such as deposits or fixed term accounts.

The directed acyclic graph below summarizes the primary causal relationships assumed in this section.

\begin{figure}
\centering
\caption{Interest Rates and Take-Up}
\vspace{0.25cm}
\tikzfig{ressources/tikz/fig2}
\end{figure}
\vspace{-1cm}

Under the structure outlined in the DAG, the lagged interest rate at period $t-1$ can be considered a valid instrument, provided that variables influenced by this rate are appropriately controlled for. The intuition behind this instrument is based on interest rate stickiness: financial products are often priced based on previously observed rates, while current market rates reflect present expectations rather than past ones. Therefore, contemporaneous demand shocks can be assumed to be uncorrelated with lagged interest rates.

This type of instrument and identification strategy is well-established in the macroeconomic literature. However, the exogeneity of the instrument cannot be formally tested. The absence of strong alternative instruments in our DATA_ADD_FULL rule out the possibility of conducting over-identification tests such as the Sargan–J test.

## Results 

```{r}

LAG_EURIBOR <- DATA_ADD_FULL |> 
  select(date_month, euribor_12mo) |> 
  unique() |> 
  mutate(euribor_12mo_l1 = lag(euribor_12mo))

REG_DATA <- DATA_ADD_FULL |> 
  left_join(LAG_EURIBOR, by= c("date_month", "euribor_12mo")) |>
  mutate(across(c(poutcome, marital, job, housing, default, education), ~as.factor(.x))) |> 
  filter(!is.na(euribor_12mo_l1)&campaign==1)

```


To estimate causal effects, three methodological approaches are employed and compared: (i) a standard two-stage least squares (2SLS) estimator applied to a linear probability model, (ii) a double machine learning (DML) estimator for a partially linear instrumental variables (IV) regression, and (iii) a probit IV regression.

\newpage

```{r}
RESULTS_MACRO <- read_feather("results_analysis/results_macro.feather")
RESULTS_MACRO |> 
  rename(Estimate = estimate, `Std Errors` = std_error, `p-value` = p_value, `Compute (s)` = compute)|> 
  mutate(name = str_remove_all(name, "IR-")) |>
  pivot_longer(-name, names_to = "Statistic") |> 
  remove_missing() |>
  pivot_wider(names_from = name, values_from = value) |>
  knitr::kable(digits = 4, align = "c", caption = "Results from IV estimation")

```


TABLE HERE 
DML and Probit are close
2SLS very different

[Discussion of 2SLS + equations]

[Discussion of DML + equations]

[Discussion of Probit + equations]

[Conclusion on results]


\newpage

# Conclusion 

## Discussion

## Limitations 


\newpage

# Bibliography

\newpage

\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}

# Appendix

## Detailed statistics 

```{r}
DATA_ADD_FULL |>
  group_by(`Marital Status` = marital) |>
  count_prop() |>
  knitr::kable(digits = 2, align = "c", caption = "Observed marital status")
```

```{r}
DATA_ADD_FULL |>
  group_by(`Marital Status` = marital) |>
  count_prop() |>
  knitr::kable(digits = 2, align = "c", caption = "Observed marital status")
```

```{r}
DATA_ADD_FULL |>
  group_by(`Education` = education) |>
  count_prop() |>
  knitr::kable(digits = 2, align = "c", caption = "Observed Job titles")
```

```{r}
DATA_ADD_FULL |>
  group_by(`Job Titles` = job) |>
  count_prop() |>
  knitr::kable(digits = 2, align = "c", caption = "Observed education")
```


## Robustness

## Robustness 

## Robustness

